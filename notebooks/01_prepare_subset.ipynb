{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ed68529-b907-4044-8387-8ded64765b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_DIR: C:\\DataProjects\\uh-ds-housing-data\\data\\raw\n",
      "INTERIM_DIR: C:\\DataProjects\\uh-ds-housing-data\\data\\interim\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to sys.path (so config.py is found)\n",
    "root = Path().resolve().parent  # one level up from notebooks/\n",
    "sys.path.append(str(root))\n",
    "\n",
    "from config import RAW_DIR, INTERIM_DIR\n",
    "import pandas as pd\n",
    "\n",
    "print(\"RAW_DIR:\", RAW_DIR)\n",
    "print(\"INTERIM_DIR:\", INTERIM_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "482ed0d5-07b0-4b3c-8247-f4f042c6531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Transaction unique identifier', 'Price', 'Date of Transfer', 'Property Type', 'Old/New', 'Duration', 'Town/City', 'District', 'County', 'PPDCategory Type', 'Record Status - monthly file only']\n",
      "Date of Transfer\n",
      "2008     650492\n",
      "2009     625662\n",
      "2010     663342\n",
      "2011     661055\n",
      "2012     668295\n",
      "2013     810111\n",
      "2014     982943\n",
      "2015    1007421\n",
      "2016    1032558\n",
      "2017     375098\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from config import RAW_DIR, INTERIM_DIR\n",
    "\n",
    "csv_path = RAW_DIR / \"price_paid_records.csv\"\n",
    "df_head = pd.read_csv(csv_path, nrows=3)\n",
    "print(df_head.columns.tolist())\n",
    "\n",
    "# sample = pd.read_csv(csv_path, nrows=5, usecols=[\"Date of Transfer\"])\n",
    "# print(sample)\n",
    "\n",
    "sample = pd.read_csv(csv_path, usecols=[\"Date of Transfer\"], parse_dates=[\"Date of Transfer\"])\n",
    "years = sample[\"Date of Transfer\"].dt.year.value_counts().sort_index()\n",
    "print(years.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8045b5da-76d6-4d2c-aa33-aea8cf5c2082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed chunk 1 — Rows kept: 0\n",
      "Processed chunk 2 — Rows kept: 0\n",
      "Processed chunk 3 — Rows kept: 0\n",
      "Processed chunk 4 — Rows kept: 0\n",
      "Processed chunk 5 — Rows kept: 0\n",
      "Processed chunk 6 — Rows kept: 0\n",
      "Processed chunk 7 — Rows kept: 0\n",
      "Processed chunk 8 — Rows kept: 0\n",
      "Processed chunk 9 — Rows kept: 0\n",
      "Processed chunk 10 — Rows kept: 0\n",
      "Processed chunk 11 — Rows kept: 0\n",
      "Processed chunk 12 — Rows kept: 0\n",
      "Processed chunk 13 — Rows kept: 0\n",
      "Processed chunk 14 — Rows kept: 0\n",
      "Processed chunk 15 — Rows kept: 0\n",
      "Processed chunk 16 — Rows kept: 0\n",
      "Processed chunk 17 — Rows kept: 0\n",
      "Processed chunk 18 — Rows kept: 0\n",
      "Processed chunk 19 — Rows kept: 0\n",
      "Processed chunk 20 — Rows kept: 0\n",
      "Processed chunk 21 — Rows kept: 0\n",
      "Processed chunk 22 — Rows kept: 0\n",
      "Processed chunk 23 — Rows kept: 0\n",
      "Processed chunk 24 — Rows kept: 0\n",
      "Processed chunk 25 — Rows kept: 0\n",
      "Processed chunk 26 — Rows kept: 0\n",
      "Processed chunk 27 — Rows kept: 0\n",
      "Processed chunk 28 — Rows kept: 0\n",
      "Processed chunk 29 — Rows kept: 0\n",
      "Processed chunk 30 — Rows kept: 0\n",
      "Processed chunk 31 — Rows kept: 0\n",
      "Processed chunk 32 — Rows kept: 0\n",
      "Processed chunk 33 — Rows kept: 211475\n",
      "Processed chunk 34 — Rows kept: 500000\n",
      "Processed chunk 35 — Rows kept: 500000\n",
      "Processed chunk 36 — Rows kept: 500000\n",
      "Processed chunk 37 — Rows kept: 500000\n",
      "Processed chunk 38 — Rows kept: 500000\n",
      "Processed chunk 39 — Rows kept: 500000\n",
      "Processed chunk 40 — Rows kept: 500000\n",
      "Processed chunk 41 — Rows kept: 500000\n",
      "Processed chunk 42 — Rows kept: 500000\n",
      "Processed chunk 43 — Rows kept: 500000\n",
      "Processed chunk 44 — Rows kept: 500000\n",
      "Processed chunk 45 — Rows kept: 489348\n",
      "\n",
      " Saved filtered subset: C:\\DataProjects\\uh-ds-housing-data\\data\\interim\\uk_housing_2010_2017.csv\n",
      "Final shape: (6200823, 10)\n"
     ]
    }
   ],
   "source": [
    "# --- File paths ---\n",
    "subset_path = INTERIM_DIR / \"uk_housing_2010_2017.csv\"\n",
    "\n",
    "# --- Columns from the dataset ---\n",
    "usecols = [\n",
    "    'Price',\n",
    "    'Date of Transfer',\n",
    "    'Property Type',\n",
    "    'Old/New',\n",
    "    'Duration',\n",
    "    'Town/City',\n",
    "    'District',\n",
    "    'County',\n",
    "    'PPDCategory Type',\n",
    "    'Record Status - monthly file only'\n",
    "]\n",
    "\n",
    "chunk_size = 500_000\n",
    "chunks = []\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(csv_path, chunksize=chunk_size, usecols=usecols)):\n",
    "    # Clean column names for consistency\n",
    "    chunk.columns = [\n",
    "        'price', 'date_of_transfer', 'property_type', 'old_new',\n",
    "        'duration', 'town_city', 'district', 'county',\n",
    "        'ppdcategory_type', 'record_status'\n",
    "    ]\n",
    "    \n",
    "    # Convert date and filter rows by year\n",
    "    chunk['date_of_transfer'] = pd.to_datetime(chunk['date_of_transfer'], errors='coerce')\n",
    "    chunk = chunk[chunk['date_of_transfer'].dt.year >= 2010]\n",
    "\n",
    "    if not chunk.empty:\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    print(f\"Processed chunk {i+1} — Rows kept: {len(chunk)}\")\n",
    "\n",
    "# Combine all filtered chunks\n",
    "df_recent = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "# Save smaller subset\n",
    "df_recent.to_csv(subset_path, index=False)\n",
    "\n",
    "print(f\"\\n Saved filtered subset: {subset_path}\")\n",
    "print(f\"Final shape: {df_recent.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53064dbe-3f35-4464-ac81-af09e3a40668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-01 2017-06-29\n",
      "(6200823, 10)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(INTERIM_DIR / \"uk_housing_2010_2017.csv\")\n",
    "print(df['date_of_transfer'].min(), df['date_of_transfer'].max())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9a7b32-7f1e-4234-bf39-2f1c14bfef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4️ Summary Statistics ===\n",
    "print(\"\\nDescriptive Statistics (Prices):\")\n",
    "print(df[\"price\"].describe())\n",
    "\n",
    "# Convert year and month for temporal analysis\n",
    "df[\"year\"] = df[\"date_of_transfer\"].dt.year\n",
    "df[\"month\"] = df[\"date_of_transfer\"].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84920c-6a5c-4d4f-b707-d31224e4299b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
