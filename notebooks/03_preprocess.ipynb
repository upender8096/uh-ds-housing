{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49abd192-f9bb-4a32-a2bf-575c689e272a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW_DIR: C:\\DataProjects\\uh-ds-housing-data\\data\\raw\n",
      "INTERIM_DIR: C:\\DataProjects\\uh-ds-housing-data\\data\\interim\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to sys.path (so config.py is found)\n",
    "root = Path().resolve().parent  # one level up from notebooks/\n",
    "sys.path.append(str(root))\n",
    "\n",
    "from config import RAW_DIR, INTERIM_DIR\n",
    "import pandas as pd\n",
    "\n",
    "print(\"RAW_DIR:\", RAW_DIR)\n",
    "print(\"INTERIM_DIR:\", INTERIM_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236137ef-6f37-467b-b13a-ac064ad5696e",
   "metadata": {},
   "source": [
    "# Step 3 — Prepare Processed Dataset\n",
    "\n",
    "**Goal:**  \n",
    "Convert the interim dataset into a clean, modeling-ready version stored in `PROCESSED_DIR`.\n",
    "\n",
    "We'll:\n",
    "1. Load the interim dataset  \n",
    "2. Handle missing values & datatypes  \n",
    "3. Engineer features (year, month, log price, high-value flag)  \n",
    "4. Save final processed dataset for modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4764e331-25e6-4648-bf48-22d0722ed78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading interim dataset from: C:\\DataProjects\\uh-ds-housing-data\\data\\interim\\uk_housing_2010_2017.csv\n",
      " Loaded 6,200,823 rows and 10 columns\n",
      " After cleaning: 5,944,841 rows (dropped 255,982)\n",
      " Date range: 2010-01-01 → 2017-06-29\n",
      " Kept 5,944,841 rows in 2010–2017 window\n",
      " High-value share: 0.201\n",
      "\n",
      " Missing Value Summary (top 10):\n",
      "Empty DataFrame\n",
      "Columns: [Missing Count, Missing %]\n",
      "Index: []\n",
      "\n",
      " Saved processed dataset to: C:\\DataProjects\\uh-ds-housing-data\\data\\processed\\uk_housing_2010_2017_processed.csv\n",
      " Final shape: (5944841, 14)\n"
     ]
    }
   ],
   "source": [
    "# === 03_prepare_processed.ipynb ===\n",
    "import pandas as pd, numpy as np\n",
    "from config import INTERIM_DIR, PROCESSED_DIR\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load exact interim file\n",
    "# -----------------------------\n",
    "interim_path = INTERIM_DIR / \"uk_housing_2010_2017.csv\"\n",
    "if not interim_path.exists():\n",
    "    raise FileNotFoundError(f\"Expected file not found: {interim_path}\")\n",
    "\n",
    "print(f\" Loading interim dataset from: {interim_path}\")\n",
    "# Read as strings to control parsing ourselves (dates/prices vary)\n",
    "df = pd.read_csv(interim_path, low_memory=False, dtype=str)\n",
    "print(f\" Loaded {len(df):,} rows and {df.shape[1]} columns\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Rename columns canonically\n",
    "# -----------------------------\n",
    "rename_map = {\n",
    "    \"Transaction unique identifier\": \"transaction_id\",\n",
    "    \"Price\": \"price\",\n",
    "    \"Date of Transfer\": \"date_of_transfer\",\n",
    "    \"Property Type\": \"property_type\",\n",
    "    \"Old/New\": \"new_build_flag\",\n",
    "    \"Duration\": \"tenure_type\",\n",
    "    \"Town/City\": \"town_city\",\n",
    "    \"District\": \"local_authority\",\n",
    "    \"County\": \"county\",\n",
    "    \"PPDCategory Type\": \"ppd_category\",\n",
    "    \"Record Status - monthly file only\": \"record_status\",\n",
    "}\n",
    "# only rename those that exist\n",
    "df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns}, inplace=True)\n",
    "\n",
    "# ensure required core columns exist\n",
    "required = [\"price\",\"date_of_transfer\"]\n",
    "missing = [c for c in required if c not in df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing required columns after rename: {missing}. Present: {df.columns.tolist()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Clean & coerce 'price'\n",
    "# -----------------------------\n",
    "# strip currency symbols/commas/spaces, then to numeric\n",
    "df[\"price\"] = (\n",
    "    df[\"price\"]\n",
    "      .astype(str)\n",
    "      .str.replace(r\"[^\\d.]\", \"\", regex=True)  # remove £, commas, spaces, etc.\n",
    "      .replace({\"\": np.nan})\n",
    ")\n",
    "df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Parse dates robustly\n",
    "# -----------------------------\n",
    "def parse_dates(s: pd.Series) -> pd.Series:\n",
    "    # Try several explicit patterns and pick the first that yields majority non-NaT\n",
    "    candidates = [\n",
    "        (\"%d/%m/%Y\", {\"dayfirst\": True}),\n",
    "        (\"%Y-%m-%d\", {}),\n",
    "        (\"%d/%m/%Y %H:%M\", {\"dayfirst\": True}),\n",
    "        (\"%Y-%m-%d %H:%M:%S\", {}),\n",
    "        (\"%d-%m-%Y\", {\"dayfirst\": True}),\n",
    "        (\"%d-%b-%Y\", {\"dayfirst\": True}),  # e.g., 01-Jan-2017\n",
    "    ]\n",
    "    best = None; best_valid = -1\n",
    "    for fmt, kw in candidates:\n",
    "        dt = pd.to_datetime(s, format=fmt, errors=\"coerce\", **kw)\n",
    "        valid = dt.notna().sum()\n",
    "        if valid > best_valid:\n",
    "            best, best_valid = dt, valid\n",
    "        if valid > 0.7 * len(s):\n",
    "            break\n",
    "    if best is None or best.notna().sum() == 0:\n",
    "        # final fallback: dateutil with dayfirst\n",
    "        best = pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
    "    return best\n",
    "\n",
    "df[\"date_of_transfer\"] = parse_dates(df[\"date_of_transfer\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Drop invalid rows & dupes\n",
    "# -----------------------------\n",
    "before = len(df)\n",
    "df = df[df[\"price\"].notna() & (df[\"price\"] > 0) & df[\"date_of_transfer\"].notna()].copy()\n",
    "\n",
    "if \"transaction_id\" in df.columns:\n",
    "    df.drop_duplicates(subset=[\"transaction_id\"], inplace=True)\n",
    "else:\n",
    "    # safe fallback de-duplication\n",
    "    subset_cols = [c for c in [\"date_of_transfer\",\"price\",\"postcode\",\"property_type\",\"town_city\",\"local_authority\",\"county\"] if c in df.columns]\n",
    "    df.drop_duplicates(subset=subset_cols if subset_cols else None, inplace=True)\n",
    "\n",
    "print(f\" After cleaning: {len(df):,} rows (dropped {before - len(df):,})\")\n",
    "if len(df) > 0:\n",
    "    print(f\" Date range: {df['date_of_transfer'].min().date()} → {df['date_of_transfer'].max().date()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Filter to 2010–2017\n",
    "# -----------------------------\n",
    "if len(df) > 0:\n",
    "    start, end = pd.Timestamp(\"2010-01-01\"), pd.Timestamp(\"2017-12-31\")\n",
    "    mask = (df[\"date_of_transfer\"] >= start) & (df[\"date_of_transfer\"] <= end)\n",
    "    kept = mask.sum()\n",
    "    df = df.loc[mask].copy()\n",
    "    print(f\" Kept {kept:,} rows in 2010–2017 window\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Feature engineering\n",
    "# -----------------------------\n",
    "if len(df) > 0:\n",
    "    df[\"year\"] = df[\"date_of_transfer\"].dt.year\n",
    "    df[\"month\"] = df[\"date_of_transfer\"].dt.month\n",
    "    df[\"log_price\"] = np.log1p(df[\"price\"])\n",
    "\n",
    "    # derive high-value = top 20% within local_authority-year (fallback to year if LA missing)\n",
    "    group_cols = [\"local_authority\",\"year\"] if \"local_authority\" in df.columns else [\"year\"]\n",
    "    df[\"price_80th\"] = df.groupby(group_cols)[\"price\"].transform(lambda s: s.quantile(0.80))\n",
    "    df[\"is_high_value\"] = (df[\"price\"] >= df[\"price_80th\"]).astype(\"Int8\")\n",
    "    df.drop(columns=[\"price_80th\"], inplace=True)\n",
    "\n",
    "    hv_share = float(df[\"is_high_value\"].mean()) if df[\"is_high_value\"].notna().any() else np.nan\n",
    "    print(\" High-value share:\", None if np.isnan(hv_share) else round(hv_share, 3))\n",
    "else:\n",
    "    # create empty columns to keep schema stable\n",
    "    for c in [\"year\",\"month\",\"log_price\",\"is_high_value\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = pd.Series(dtype=\"float64\" if c!=\"is_high_value\" else \"Int8\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Missing summary (top 10)\n",
    "# -----------------------------\n",
    "if len(df) > 0:\n",
    "    na_counts = df.isna().sum()\n",
    "    na_pct = (na_counts / len(df) * 100).round(2)\n",
    "    miss = pd.DataFrame({\"Missing Count\": na_counts, \"Missing %\": na_pct})\n",
    "    print(\"\\n Missing Value Summary (top 10):\")\n",
    "    print(miss[miss[\"Missing %\"] > 0].sort_values(\"Missing %\", ascending=False).head(10))\n",
    "else:\n",
    "    print(\"\\n No rows after cleaning. Check date parsing and price coercion above.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9) Save processed CSV\n",
    "# -----------------------------\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_path = PROCESSED_DIR / \"uk_housing_2010_2017_processed.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"\\n Saved processed dataset to: {out_path}\")\n",
    "print(f\" Final shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb590ba-d38a-4de1-8669-8982647493b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
